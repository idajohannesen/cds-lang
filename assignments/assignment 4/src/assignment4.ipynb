{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict emotion scores for all lines in the data\n",
    "for every season, plot distribution of all emotion labels in that season\n",
    "for each emotion label, plot relative freq of each emotion across all seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load classifier pipeline from HuggingFace\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model=\"j-hartmann/emotion-english-distilroberta-base\", \n",
    "                      return_all_scores=False) # return only the most likely score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigating into the input folder and reading the dataset as a pandas dataframe\n",
    "filename = os.path.join(\"../input/Game_of_Thrones_Script.csv\")\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Release Date    Season    Episode     Episode Title              Name  \\\n",
      "0       2011-04-17  Season 1  Episode 1  Winter is Coming      waymar royce   \n",
      "1       2011-04-17  Season 1  Episode 1  Winter is Coming              will   \n",
      "2       2011-04-17  Season 1  Episode 1  Winter is Coming      waymar royce   \n",
      "3       2011-04-17  Season 1  Episode 1  Winter is Coming              will   \n",
      "4       2011-04-17  Season 1  Episode 1  Winter is Coming             gared   \n",
      "...            ...       ...        ...               ...               ...   \n",
      "23906   2019-05-19  Season 8  Episode 6   The Iron Throne           brienne   \n",
      "23907   2019-05-19  Season 8  Episode 6   The Iron Throne             bronn   \n",
      "23908   2019-05-19  Season 8  Episode 6   The Iron Throne  tyrion lannister   \n",
      "23909   2019-05-19  Season 8  Episode 6   The Iron Throne               man   \n",
      "23910   2019-05-19  Season 8  Episode 6   The Iron Throne               all   \n",
      "\n",
      "                                                Sentence  \n",
      "0      What do you expect? They're savages. One lot s...  \n",
      "1      I've never seen wildlings do a thing like this...  \n",
      "2                                 How close did you get?  \n",
      "3                                Close as any man would.  \n",
      "4                       We should head back to the wall.  \n",
      "...                                                  ...  \n",
      "23906  I think we can all agree that ships take prece...  \n",
      "23907      I think that's a very presumptuous statement.  \n",
      "23908  I once brought a jackass and a honeycomb into ...  \n",
      "23909                            The Queen in the North!  \n",
      "23910  The Queen in the North! The Queen in the North...  \n",
      "\n",
      "[23911 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        What do you expect? They're savages. One lot s...\n",
      "1        I've never seen wildlings do a thing like this...\n",
      "2                                   How close did you get?\n",
      "3                                  Close as any man would.\n",
      "4                         We should head back to the wall.\n",
      "                               ...                        \n",
      "23906    I think we can all agree that ships take prece...\n",
      "23907        I think that's a very presumptuous statement.\n",
      "23908    I once brought a jackass and a honeycomb into ...\n",
      "23909                              The Queen in the North!\n",
      "23910    The Queen in the North! The Queen in the North...\n",
      "Name: Sentence, Length: 23911, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data[\"Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'disgust', 'score': 0.8400837182998657}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(data[\"Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/Language Analytics/cds-lang/assignments/assignment 4/src/assignment4.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m datastr:\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     result \u001b[39m=\u001b[39m classifier(row)\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     scores\u001b[39m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,)\n\u001b[0;32m--> 156\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    836\u001b[0m     embedding_output,\n\u001b[1;32m    837\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    838\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    839\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    840\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    841\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    842\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    843\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    844\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    845\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    414\u001b[0m         hidden_states,\n\u001b[1;32m    415\u001b[0m         attention_mask,\n\u001b[1;32m    416\u001b[0m         head_mask,\n\u001b[1;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m    343\u001b[0m         head_mask,\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    346\u001b[0m         past_key_value,\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:220\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 220\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[1;32m    222\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    224\u001b[0m use_cache \u001b[39m=\u001b[39m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for row in datastr:\n",
    "    result = classifier(row)\n",
    "    scores.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/work/Language Analytics/cds-lang/assignments/assignment 4/src/assignment4.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(scores[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "#print(scores[0])\n",
    "print(scores[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Release Date    Season    Episode     Episode Title              Name  \\\n",
      "0       2011-04-17  Season 1  Episode 1  Winter is Coming      waymar royce   \n",
      "1       2011-04-17  Season 1  Episode 1  Winter is Coming              will   \n",
      "2       2011-04-17  Season 1  Episode 1  Winter is Coming      waymar royce   \n",
      "3       2011-04-17  Season 1  Episode 1  Winter is Coming              will   \n",
      "4       2011-04-17  Season 1  Episode 1  Winter is Coming             gared   \n",
      "...            ...       ...        ...               ...               ...   \n",
      "23906   2019-05-19  Season 8  Episode 6   The Iron Throne           brienne   \n",
      "23907   2019-05-19  Season 8  Episode 6   The Iron Throne             bronn   \n",
      "23908   2019-05-19  Season 8  Episode 6   The Iron Throne  tyrion lannister   \n",
      "23909   2019-05-19  Season 8  Episode 6   The Iron Throne               man   \n",
      "23910   2019-05-19  Season 8  Episode 6   The Iron Throne               all   \n",
      "\n",
      "                                                Sentence  \n",
      "0      What do you expect? They're savages. One lot s...  \n",
      "1      I've never seen wildlings do a thing like this...  \n",
      "2                                 How close did you get?  \n",
      "3                                Close as any man would.  \n",
      "4                       We should head back to the wall.  \n",
      "...                                                  ...  \n",
      "23906  I think we can all agree that ships take prece...  \n",
      "23907      I think that's a very presumptuous statement.  \n",
      "23908  I once brought a jackass and a honeycomb into ...  \n",
      "23909                            The Queen in the North!  \n",
      "23910  The Queen in the North! The Queen in the North...  \n",
      "\n",
      "[23911 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "dftest = data\n",
    "# Using DataFrame.insert() to add a column\n",
    "df.insert(2, \"Emotion label\", scores, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Season\n",
       "Season 2    3914\n",
       "Season 3    3573\n",
       "Season 4    3446\n",
       "Season 1    3179\n",
       "Season 5    3035\n",
       "Season 6    2856\n",
       "Season 7    2442\n",
       "Season 8    1466\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1scores = []\n",
    "for row in s1[\"Sentence\"][0:10]:\n",
    "    result = classifier(row)\n",
    "    s1scores.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'disgust', 'score': 0.8400837182998657}], [{'label': 'disgust', 'score': 0.6117002367973328}], [{'label': 'surprise', 'score': 0.7143816947937012}], [{'label': 'neutral', 'score': 0.9114254713058472}], [{'label': 'fear', 'score': 0.42081132531166077}], [{'label': 'fear', 'score': 0.9559083580970764}], [{'label': 'neutral', 'score': 0.6202900409698486}], [{'label': 'surprise', 'score': 0.7657095193862915}], [{'label': 'anger', 'score': 0.5590818524360657}], [{'label': 'anger', 'score': 0.6356110572814941}]]\n"
     ]
    }
   ],
   "source": [
    "print(s1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = data[data['Season'] == 'Season 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Episode Title</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-04-17</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 1</td>\n",
       "      <td>Winter is Coming</td>\n",
       "      <td>waymar royce</td>\n",
       "      <td>What do you expect? They're savages. One lot s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-04-17</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 1</td>\n",
       "      <td>Winter is Coming</td>\n",
       "      <td>will</td>\n",
       "      <td>I've never seen wildlings do a thing like this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-17</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 1</td>\n",
       "      <td>Winter is Coming</td>\n",
       "      <td>waymar royce</td>\n",
       "      <td>How close did you get?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-17</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 1</td>\n",
       "      <td>Winter is Coming</td>\n",
       "      <td>will</td>\n",
       "      <td>Close as any man would.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-04-17</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 1</td>\n",
       "      <td>Winter is Coming</td>\n",
       "      <td>gared</td>\n",
       "      <td>We should head back to the wall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>2011-06-19</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 10</td>\n",
       "      <td>Fire and Blood</td>\n",
       "      <td>daenerys targaryen</td>\n",
       "      <td>ls that what you fear? You will be my khalasar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>2011-06-19</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 10</td>\n",
       "      <td>Fire and Blood</td>\n",
       "      <td>daenerys targaryen</td>\n",
       "      <td>Ser Jorah, bind this woman to the pyre. You sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>2011-06-19</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 10</td>\n",
       "      <td>Fire and Blood</td>\n",
       "      <td>mirri maz duur</td>\n",
       "      <td>You will not hear me scream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>2011-06-19</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 10</td>\n",
       "      <td>Fire and Blood</td>\n",
       "      <td>daenerys targaryen</td>\n",
       "      <td>I will. But it is not your screams I want. Onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>2011-06-19</td>\n",
       "      <td>Season 1</td>\n",
       "      <td>Episode 10</td>\n",
       "      <td>Fire and Blood</td>\n",
       "      <td>jorah mormont</td>\n",
       "      <td>Blood of my blood.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3179 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Release Date    Season     Episode     Episode Title                Name  \\\n",
       "0      2011-04-17  Season 1   Episode 1  Winter is Coming        waymar royce   \n",
       "1      2011-04-17  Season 1   Episode 1  Winter is Coming                will   \n",
       "2      2011-04-17  Season 1   Episode 1  Winter is Coming        waymar royce   \n",
       "3      2011-04-17  Season 1   Episode 1  Winter is Coming                will   \n",
       "4      2011-04-17  Season 1   Episode 1  Winter is Coming               gared   \n",
       "...           ...       ...         ...               ...                 ...   \n",
       "3174   2011-06-19  Season 1  Episode 10    Fire and Blood  daenerys targaryen   \n",
       "3175   2011-06-19  Season 1  Episode 10    Fire and Blood  daenerys targaryen   \n",
       "3176   2011-06-19  Season 1  Episode 10    Fire and Blood      mirri maz duur   \n",
       "3177   2011-06-19  Season 1  Episode 10    Fire and Blood  daenerys targaryen   \n",
       "3178   2011-06-19  Season 1  Episode 10    Fire and Blood       jorah mormont   \n",
       "\n",
       "                                               Sentence  \n",
       "0     What do you expect? They're savages. One lot s...  \n",
       "1     I've never seen wildlings do a thing like this...  \n",
       "2                                How close did you get?  \n",
       "3                               Close as any man would.  \n",
       "4                      We should head back to the wall.  \n",
       "...                                                 ...  \n",
       "3174  ls that what you fear? You will be my khalasar...  \n",
       "3175  Ser Jorah, bind this woman to the pyre. You sw...  \n",
       "3176                       You will not hear me scream.  \n",
       "3177  I will. But it is not your screams I want. Onl...  \n",
       "3178                                 Blood of my blood.  \n",
       "\n",
       "[3179 rows x 6 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (693) does not match length of index (3180)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/work/Language Analytics/cds-lang/assignments/assignment 4/src/assignment4.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dftest \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m:\u001b[39m3180\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Using DataFrame.insert() to add a column\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5037761-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dftest\u001b[39m.\u001b[39;49minsert(\u001b[39m2\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mEmotion label\u001b[39;49m\u001b[39m\"\u001b[39;49m, s1scores, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:5171\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5168\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   5169\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m-> 5171\u001b[0m value, refs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   5172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39minsert(loc, column, value, refs\u001b[39m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   5267\u001b[0m arr \u001b[39m=\u001b[39m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[39misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[39m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[39m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (693) does not match length of index (3180)"
     ]
    }
   ],
   "source": [
    "dftest = data[0:3180]\n",
    "# Using DataFrame.insert() to add a column\n",
    "dftest.insert(2, \"Emotion label\", s1scores, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,6))\n",
    "plot = sns.barplot(x = season 1 emotion labels.index, y = episodes.values)\n",
    "plot.set_title('Episodes per season')\n",
    "plot.set_xlabel('Seasons')\n",
    "plot.set_ylabel('Episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'neutral', 'score': 0.8145653009414673}], [{'label': 'fear', 'score': 0.7027424573898315}]]\n"
     ]
    }
   ],
   "source": [
    "testscores = []\n",
    "for row in datastr[:2]:\n",
    "    result = classifier(row)\n",
    "    testscores.append(result)\n",
    "print(testscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "print(testscores[0][0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "7 columns passed, passed data had 14 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[39mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 7 columns passed, passed data had 14 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/work/Language Analytics/cds-lang/assignments/assignment 4/src/assignment4.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5035253-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame([teststring], columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39manger\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdisgust\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mjoy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mneutral\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msadness\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msurprise\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5035253-0.cloud.sdu.dk/work/Language%20Analytics/cds-lang/assignments/assignment%204/src/assignment4.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    852\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m         data,\n\u001b[1;32m    855\u001b[0m         columns,\n\u001b[1;32m    856\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m         dtype,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[39m=\u001b[39m convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 7 columns passed, passed data had 14 columns"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([teststring], columns=[\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
