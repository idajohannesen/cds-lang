{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Evaluating environmental impact of your exam portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emissions have been measured through CodeCarbon as kilograms of CO₂-equivalent (CO₂eq). The first part of the notebook compares the emissions for each assignment overall and discusses these results. Afterwards, every assignment is presented and discussed one by one. The discussions are accompanied by csv files and graphs with the emissions for every task included in the assignment. This choice was made because the overall emissions for a single assignment does not reveal anything about which specific tasks have a bigger impact on the level of CO₂eq being emitted. Without looking at the individual tasks, it is hard to suggest improvements to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for all assignments\n",
    "\n",
    "filename = os.path.join(\"../input/emissions.csv\")\n",
    "emissions = pd.read_csv(filename)\n",
    "emissions[[\"project_name\", \"emissions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions.plot(x=\"project_name\", y=\"emissions\", kind=\"bar\") \n",
    "plt.title(\"All assignments\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/overall_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion of results\n",
    "\n",
    "Between all 4 assignments, assignment 4 generated by far the most CO₂eq overall. Assignment 1 is next, and while some of the scripts for assignment 2 had less emissions, combining all three means that assignment 2 generated more emissions than assignment 3, which had the lowest number.\n",
    "\n",
    "Based on these results, Assignment 4 has the largest environment impact. The cause of this and ways to reduce the emissions will be discussed in the later section dedicated to Assignment 4.\n",
    "\n",
    "Even then, these numbers are only for a single run through the scripts. In reality, while writing and testing the code, each assignment has been run a number of times which multiplies the number of emissions several times. This is also an opportunity for reducing the environmental impact. When writing the code in the first place, one could consider using a smaller dataset while troubleshooting or commenting out heavier tasks when checking if later, less intense tasks are working. By doing so, running the script would hopefully generate less emissions than using the full dataset and code. Additionally, manually looking over the code for easily spotted mistakes such as inconsistent variables or typos means that the code can run succesfully with fewer trial runs.\n",
    "\n",
    "Additionally, these results are subject to a certain level of variation in the generation of emissions, as no two runs of the same script will necessarily generate the same amount of CO₂eq. Note that there are two runs of assignment 1 included in the visualization and csv file for this reason. Despite being the same script, they were tracked to have different amounts of emissions generated. The tracking is therefore not 100% robust as these two runs have a variation of approximately 0.0003 kilograms of CO₂eq. Larger and more noticable variations may occur, even if this one is quite small.\n",
    "\n",
    "One additional comment is that the emissions tracker does not track the installation or loading of the CodeCarbon package itself nor the os package which is used in setting up the tracker. It should therefore be kept in mind that these tasks also generate emissions. Every other package is being tracked as it is imported, and this task is often one of the more intensive tasks apart from tasks related to machine learning or large for loops. However, importing packages is a necessary step for all of the assignments and must be redone every time a new instance of UCloud is started or a new virtual environment is created. It is therefore difficult to make this task generate fewer emissions.\n",
    "\n",
    "\n",
    "Following this, we look at every assignment and it's tasks further in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for assignment 1\n",
    "\n",
    "filename1 = os.path.join(\"../input/emissions_base_b1578009-007e-48b2-943e-96d64586147f.csv\")\n",
    "assignment1 = pd.read_csv(filename1)\n",
    "assignment1 = assignment1[[\"task_name\", \"emissions\"]]\n",
    "assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment1.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\") \n",
    "plt.title(\"Assignment 1\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/Assignment1_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion of results\n",
    "\n",
    "For the first assignment, extracting linguistic features from the texts is the most demanding task, in terms of emissions. Loading the model also generates a small amount, but this is a necessary step and it is hard to limit the emissions here. Lastly, the simple tasks of navigating and sorting directories as well as plotting and calculating averages are generating a negligible amount of CO₂eq. This will be a general trend throughout all four assignments.\n",
    "\n",
    "The task named ```extract features``` refers to a for loop which both reads through every file and extracts linguistic features. This includes a counter of various word classes and named entities. Because of the structure, it is hard to pinpoint which part of the task is generating the most CO₂eq. It is also hard to split it up as the loop needs to count several features, and splitting it would require reading the files once per task which would only add to the assignment overall being more intensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the setup with three separate scripts, this section will also be split up into a dataframe and graph of each script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for assignment 2's vectorizer\n",
    "\n",
    "filename2vec = os.path.join(\"../input/emissions_base_e9cd3e0f-665b-4ef7-87af-3af5b38b9176.csv\")\n",
    "assignment2vec = pd.read_csv(filename2vec)\n",
    "assignment2vec[[\"task_name\", \"emissions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment2vec.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\")\n",
    "plt.title(\"Assignment 2 vectorizer\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\") \n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/Assignment2vec_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for assignment 2's logistic regression classifier\n",
    "\n",
    "filename2logreg = os.path.join(\"../input/emissions_base_b5cae586-a113-4589-9cac-05563ef18571.csv\")\n",
    "assignment2logreg = pd.read_csv(filename2logreg)\n",
    "assignment2logreg[[\"task_name\", \"emissions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment2logreg.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\") \n",
    "plt.title(\"Assignment 2 logrec\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/Assignment2logreg_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for assignment 2's mlp classifier\n",
    "\n",
    "filename2mlp = os.path.join(\"../input/emissions_base_aa051452-02ad-46c0-aef9-240620a8e2b6.csv\")\n",
    "assignment2mlp = pd.read_csv(filename2mlp)\n",
    "assignment2mlp[[\"task_name\", \"emissions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment2mlp.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\") \n",
    "plt.title(\"Assignment 2 mlp\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/Assignment2mlp_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all three scripts combined into one visualization\n",
    "\n",
    "assignment2 = pd.concat([assignment2vec, assignment2logreg, assignment2mlp])\n",
    "\n",
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment2.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\", figsize = (16,6)) \n",
    "plt.title(\"Assignment 2\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion of results\n",
    "\n",
    "Between the three scripts, the logistic regression classifier generates the most emissions, followed by the mlp classifier. This is mainly due to the cross validation graph, as the mlp classifier actually emits considerably more CO₂eq compared to the logistic regression classifier. Vectorizing the data results a noticable amount of emissions for both classifiers. Even thought creating the vectorizer generates very little CO₂eq already, having it as a separate script means that it only has to be run once for both classifiers to use it, which reduces overall emissions. \n",
    "\n",
    "As for individual tasks, fitting the vectorizer and specifically the mlp classifier also leads to relatively high numbers of CO₂eq being emitted. Up until the classifier fitting, the logistic regression and mlp scripts are identical. To reduce emissions you could either chose to only run one of the classifiers, or you could combine them both in one script and only have to perform the first few stephs of loading the data and fitting the vectorizer once.\n",
    "\n",
    "The different performance visualizations also generate a bit of CO₂eq, especially the cross validation graph. As these are just to doublecheck performance and do not actually affect it in any way, they could be left out in any runs that do not include any tweaking of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for assignment 3\n",
    "\n",
    "filename3 = os.path.join(\"../input/emissions_base_f6b8a607-157f-4e1a-9eb6-595842378d92.csv\")\n",
    "assignment3 = pd.read_csv(filename3)\n",
    "assignment3[[\"task_name\", \"emissions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment3.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\") \n",
    "plt.title(\"Assignment 3\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/Assignment3_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion of results\n",
    "\n",
    "While Assignment 3 overall has the lowest emissions, the above plot shows that loading the model is the most intensive task in this script. The model in question is a pretrained model, which does mean that no training is required. The model choice is therefore already one with lower emissions. \n",
    "\n",
    "This assignment is already fairly optimized in terms of avoiding unecessary emissions. This might also be because the dataset is limited by both an artist and specific keywords when searching through and counting how many instances of the input word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv file with emissions for assignment 4\n",
    "\n",
    "filename4 = os.path.join(\"../input/emissions_base_de9b8bd9-5e27-44e1-a5a4-cf6a487ec694.csv\")\n",
    "assignment4 = pd.read_csv(filename4)\n",
    "assignment4[[\"task_name\", \"emissions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a visualization of the emissions\n",
    "\n",
    "assignment4.plot(x=\"task_name\", y=\"emissions\", kind=\"bar\") \n",
    "plt.title(\"Assignment 4\")\n",
    "plt.ylabel(\"Emissions\")\n",
    "plt.xlabel(\"Task names\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.savefig('../output/Assignment4_emissions.png') # save output\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion of results\n",
    "\n",
    "As previously mentioned, Assignment 4 generates more CO₂eq than any other assignment. As shown by the visualization above, this is due to the sentiment analysis.\n",
    "\n",
    "The ```sentiment analysis``` task performs a sentiment analysis of the entire Game of Thrones manuscript, which is a task that requires a lot of time and resources as it loops over every line and provides an emotion label. Therefore, it comes as no surprise that it generates the most emissions. \n",
    "In order to lower this number, one suggestion would be to reduce the dataset. Instead of looking at every single line of dialogue, you could focus on certain seasons, scenes, or characters and only run the sentiment analysis for those lines. \n",
    "Alternatively, if the overall emotion is still of interest, you could instead cut out any line that falls below a certain amount of characters. This would skip lines with very simple dialogue, yes/no answers, or names being called, which are usually analyzed as being neutral anyways. This would skew the amount of neutral labels being used, but you might question if these short replies are even being classified correctly in the first place, since they contain very limited information for the model to go on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_assignment",
   "language": "python",
   "name": "env_assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
